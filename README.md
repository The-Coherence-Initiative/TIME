# TIME: Temporally Intelligent Meta-reasoning Engine for Chronologically Aware Language Models (Training and Datasets)

Reproducible training pipeline for TIME models across phases with QLoRA finetuning. This repo contains the Jupyter notebooks, scripts, and phase data splits used to recreate the paper’s training results.

---

## Installation

Use Python 3.12+ and install:

```bash
pip install torch==2.9.1 unsloth==2025.12.8 transformers==4.57.3 accelerate==1.12.0 numpy==1.26.4 xformers==0.0.33.post2 scipy==1.16.3
````

You’ll also want a Jupyter environment (e.g., `jupyter` or VS Code) to run the notebooks.

---

## Directory layout

```
.
└── Data_Sampling.ipynb
├── Trainer-4B.ipynb
├── Trainer-8B.ipynb
├── Trainer-14B.ipynb
├── Trainer-32B.ipynb
├── phase1.json
├── phase2.json
├── phase3.json
├── phase1_train.json
├── phase2_train.json
├── phase3_train.json
├── phase1_test.json
├── phase2_test.json
└── phase3_test.json
```

> The `{phase}_train.json` / `{phase}_test.json` files are either generated by `Data_Sampling.ipynb` or provided as pre-generated splits (see below).

---

## Workflow

### 1) Data preparation (phase-wise splits + replay)

Open `Training/Data_Sampling.ipynb`:

* Generates **train/test** splits for `phase{1,2,3}.json`
* Appends replay data as configured
* Uses sampling seeds **42** and **43**

> Pre-generated splits are included. You can skip this step if you’re only reproducing evaluations.

### 2) Model finetuning (QLoRA 4-bit)

Pick the appropriate notebook per model size:

* `Trainer-4B.ipynb`
* `Trainer-8B.ipynb`
* `Trainer-14B.ipynb`
* `Trainer-32B.ipynb`

Inside each trainer:

* Training seed is **3407** (modifiable in-notebook)
* All hyperparameters (optimizer, batch size, epochs, grad accumulation, scheduler) are explicitly visible and editable
* Check VRAM guidance:

  * **32B**: recommend **≥ 40GB** VRAM
  * **4B/8B/14B**: trainable on **24GB** VRAM

**Determinism**: Loss metrics match within **two decimal places** across supported hardware; expect small floating-point variations.


---

## Inference note (vLLM)

While this repo focuses on training, our evaluations used `vllm==0.13.0`. Install it separately if you intend to run inference locally:

```bash
pip install vllm==0.13.0
```

---

## Reproducibility and limitations

* All intermediate `.json` artifacts needed to resume the pipeline at any stage are included.
* **No model checkpoints** are provided. The notebooks + exact configurations are sufficient to **recreate checkpoints faithfully**.
* For evaluation seeds/statistics, see the TIMEBench repo (the judging/statistics scripts accept `--seed`).

---

## Reference hardware (for transparency)

* CPU: AMD Ryzen 9 7950X3D
* RAM: 128GB DDR5
* GPU: NVIDIA RTX Pro 6000 Blackwell (96GB VRAM)
* Driver/CUDA: 582.08 / CUDA 13.0
