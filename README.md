# TIME: Temporally Intelligent Meta-reasoning Engine for Chronologically Aware Language Models (Training and Datasets)

Reproducible training pipeline for TIME models across phases, with QLoRA finetuning and post-training FP8 conversion. This repo contains the Jupyter notebooks, scripts, and phase data splits used to recreate the paper’s training results.

---

## Installation

Use Python 3.10+ and install:

```bash
pip install \
  unsloth==2025.5.7 \
  transformers==4.51.3 \
  accelerate==1.7.0 \
  numpy==1.26.4 \
  llmcompressor==0.6.0 \
  scipy==1.15.3
````

You’ll also want a Jupyter environment (e.g., `jupyter` or VS Code) to run the notebooks.

---

## Directory layout

```
.
└── Data_Sampling.ipynb
├── Trainer-4B.ipynb
├── Trainer-8B.ipynb
├── Trainer-14B.ipynb
├── Trainer-32B.ipynb
├── convert_to_fp8.py
├── phase1.json
├── phase2.json
├── phase3.json
├── phase1_train.json
├── phase2_train.json
├── phase3_train.json
├── phase1_test.json
├── phase2_test.json
└── phase3_test.json
```

> The `{phase}_train.json` / `{phase}_test.json` files are either generated by `Data_Sampling.ipynb` or provided as pre-generated splits (see below).

---

## Workflow

### 1) Data preparation (phase-wise splits + replay)

Open `Training/Data_Sampling.ipynb`:

* Generates **train/test** splits for `phase{1,2,3}.json`
* Appends replay data as configured
* Uses sampling seeds **42** and **43**

> Pre-generated splits are included. You can skip this step if you’re only reproducing evaluations.

### 2) Model finetuning (QLoRA 4-bit)

Pick the appropriate notebook per model size:

* `Trainer-4B.ipynb`
* `Trainer-8B.ipynb`
* `Trainer-14B.ipynb`
* `Trainer-32B.ipynb`

Inside each trainer:

* Training seed is **3407** (modifiable in-notebook)
* All hyperparameters (optimizer, batch size, epochs, grad accumulation, scheduler) are explicitly visible and editable
* Check VRAM guidance:

  * **32B**: recommend **≥ 40GB** VRAM
  * **4B/8B/14B**: trainable on **24GB** VRAM

**Determinism**: Loss metrics match within **two decimal places** across supported hardware; expect small floating-point variations.

### 3) FP8 conversion (dynamic e4m3)

Once finetuning completes, quantize to FP8 with:

```bash
python Training/convert_to_fp8.py --model <path-or-hf-id-of-your-trained-model>
```

**Hardware requirement** for downstream FP8 inference with vLLM: NVIDIA GPU with **compute capability ≥ 8.9** (Ada, Hopper, Blackwell).

---

## Inference note (vLLM)

While this repo focuses on training, our evaluations used `vllm==0.10.0` with FP8 decoding. Install it separately if you intend to run inference locally:

```bash
pip install vllm==0.10.0
```

---

## Reproducibility and limitations

* All intermediate `.json` artifacts needed to resume the pipeline at any stage are included.
* **No model checkpoints** are provided. The notebooks + exact configurations are sufficient to **recreate checkpoints faithfully**, matching trainer-side loss statistics within two decimals.
* For evaluation seeds/statistics, see the TIMEBench repo (the judging/statistics scripts accept `--seed`).

---

## Reference hardware (for transparency)

* CPU: AMD Ryzen 9 7950X3D
* RAM: 128GB DDR5
* GPU: NVIDIA RTX 6000 Ada (48GB VRAM)
* Driver/CUDA: 576.02 / CUDA 12.9
