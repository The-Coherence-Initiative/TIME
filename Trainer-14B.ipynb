{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a288b1-567e-4c3d-ab66-fe26422586fb",
   "metadata": {},
   "source": [
    "# Environment Setup: Package Installation\n",
    "\n",
    "This cell installs all required Python packages for the training and quantization pipeline.  \n",
    "- **unsloth** (2025.5.7): Core framework for efficient QLoRA training and inference.\n",
    "- **transformers** (4.51.3), **accelerate** (1.7.0): Model loading, tokenization, and hardware acceleration.\n",
    "- **numpy** (1.26.4): Array operations for preprocessing and metric computation.\n",
    "- **llmcompressor** (0.6.0): Used exclusively for FP8 conversion and quantization prior to downstream inference (not required for core training).\n",
    "- **scipy** (1.15.3): Required only for post-training statistical analysis (e.g., Wilcoxon tests).\n",
    "\n",
    "> **Note:**  \n",
    "> This cell is only required on a fresh environment or the first run. If all dependencies are already installed, it can be safely skipped.  \n",
    ">  \n",
    "> **llmcompressor** and **scipy** are *not* required for core model training, but are included here to avoid potential version mismatch issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca44a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch==2.9.1 unsloth==2025.12.8 transformers==4.57.3 accelerate==1.12.0 numpy==1.26.4 xformers==0.0.33.post2 scipy==1.16.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e0f7e-fcf5-426e-9100-9b3b86042865",
   "metadata": {},
   "source": [
    "# Notebook Structure and Usage Guide\n",
    "\n",
    "This notebook is organized into **five sections**:\n",
    "1. **Preliminary Setup** (imports, random seeds, model selection)\n",
    "2. **Phase 1 Training**\n",
    "3. **Phase 2 Training**\n",
    "4. **Phase 3 Training**\n",
    "5. **Phase 4 (Full-Batch Alignment)**\n",
    "\n",
    "**Execution Guidance:**\n",
    "- Each phase can be run independently in separate sessions or kernel instances, *provided that the output artifacts (models/checkpoints) from all prior phases are available* in the same workspace.\n",
    "- If preferred, the full notebook can be executed sequentially (‚ÄúRun All‚Äù), running the entire pipeline start to finish in one kernel instance.\n",
    "- The **Preliminary Setup** section must always be run at the start of every session, as it initializes the environment, imports required libraries, and sets random seeds.\n",
    "\n",
    "> *Tip:*  \n",
    "> If you are resuming after an interruption or switching kernel, re-run the **Preliminary Setup** and then proceed directly to the phase of interest, as long as previous phases have already been completed and their outputs saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b0332-f69d-4241-a2fb-5a912010bf7f",
   "metadata": {},
   "source": [
    "# Preliminary Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1159a-0fb0-445c-903e-6644873aede1",
   "metadata": {},
   "source": [
    "### Environment and Random Seed Initialization\n",
    "\n",
    "This cell imports all necessary libraries, sets up deterministic random seeds for full reproducibility, and configures PyTorch CUDA settings (if available).\n",
    "\n",
    "- `unsloth`, `transformers`, `trl`, and `peft` provide the core fine-tuning and adapter infrastructure.\n",
    "- `numpy` and Python‚Äôs `random` standard library are used for explicit seed management.\n",
    "- `Path` is used for consistent file operations.\n",
    "- All seeds are set to a fixed value (`3407`) to ensure experiment reproducibility across runs and environments.\n",
    "\n",
    "*You must run this cell at the start of every notebook session or after resetting the kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da548a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/susmit/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "WARNING 01-03 08:37:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "dtype = torch.float16\n",
    "n_gpus = torch.cuda.device_count()\n",
    "\n",
    "GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n",
    "\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import set_seed, AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 3407\n",
    "set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf76cb-f182-4980-b221-b6474fe92c6a",
   "metadata": {},
   "source": [
    "### Model Selection and Data Loading Utilities\n",
    "\n",
    "- `model_name` sets the base model checkpoint to use (e.g., `\"Qwen/Qwen3-14B\"`). You can adjust this to select among supported Qwen3 sizes.\n",
    "- The `load_split()` utility function loads training and test data splits from local JSON files (expects UTF-8 encoding).\n",
    "- This modular loader will be used by each phase to ingest its own split of curriculum/alignment data.\n",
    "\n",
    "*Adjust `model_name` if running on a different scale of Qwen3, and ensure the required data files are present in your workspace.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9b2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "#Data loader\n",
    "def load_split(train_path, test_path):\n",
    "    with open(train_path, 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(test_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb934f5-bfd1-4135-97b1-de5cd84e5135",
   "metadata": {},
   "source": [
    "# Phase 1: Single-Turn TIME Interventions (Structured Pre-Reasoning)\n",
    "\n",
    "This phase introduces models to structured temporal inputs and expected outputs within **single-turn interactions**. The purpose is to *prime* the model toward sensitivity to:\n",
    "\n",
    "- Temporal tags (e.g., `<time>`)\n",
    "- Multiple reasoning block slots (`<think>...</think>`)\n",
    "\n",
    "This aims to encourage surface imitation before deeper semantic reasoning is introduced.\n",
    "\n",
    "> The actual outputs from the author's training runs are preserved in the notebook for transparency and auditability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fff4b-0924-4ff1-93c5-e5a821aae99f",
   "metadata": {},
   "source": [
    "### Load Phase 1 Dataset\n",
    "\n",
    "This cell loads the preprocessed JSON files for Phase 1‚Äîdesigned for single-turn conversations only‚Äîinto memory.\n",
    "\n",
    "- Each entry is a fully-formed prompt-response example, structured as chat messages.\n",
    "- The files must be named `phase1_train.json` and `phase1_test.json`, and must already be formatted with roles (`user`, `assistant`) and timestamps.\n",
    "\n",
    "The print output confirms successful loading and gives visibility into the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5064799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2188 train and 387 test conversations.\n"
     ]
    }
   ],
   "source": [
    "# Load phase 1 data\n",
    "train_data, test_data = load_split('phase1_train.json', 'phase1_test.json')\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train and {len(test_data)} test conversations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2be2c4-f3b4-4f3d-99d3-8c5f2257d76e",
   "metadata": {},
   "source": [
    "### Initialize Tokenizer and Chat Template\n",
    "\n",
    "This cell initializes the tokenizer from the selected base model (e.g., `Qwen/Qwen3-14B`) and configures a custom `chat_template` that emulates the expected format during fine-tuning and downstream inference.\n",
    "\n",
    "- The template ensures that timestamps, assistant formatting and responses are in line with our framework.\n",
    "- This structure mirrors the Qwen3 convention, with just TIME-specific modifications such as `<time>` and `<think>` tags added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7296a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.chat_template = \"\"\"{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\'}}\\n        {%- if message.role == \"user\" and message.timestamp is defined %}\\n            {{- \\'<time>\\' + message.timestamp + \\'</time>\\\\n\\' }}\\n        {%- endif %}\\n        {{- content + \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\\\n\\' }}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e64e16-b585-4afc-affa-2eeb706990a5",
   "metadata": {},
   "source": [
    "### Apply Chat Template to Dataset\n",
    "\n",
    "This applies the above `chat_template` to both training and testing splits, generating **raw string prompts** (without tokenization yet).\n",
    "\n",
    "- This lets us visually inspect how system, user, and assistant roles are represented.\n",
    "- The outputs printed in this cell are the *first 500 characters* of the first training and test samples, giving visibility into whether formatting is preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e53b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First formatted train example:\n",
      "<|im_start|>system\n",
      "You are an AI assistant. Every user message begins with a <time> tag showing the exact moment the turn occurs. Sometimes, there's only the timestamp with no text‚Äîthat means time advanced without user input. Use <think>...</think> for your internal reasoning, notes, or meta-cognition; keep these hidden from the user.<|im_end|>\n",
      "<|im_start|>user\n",
      "<time>2023-11-01T10:56:00</time>\n",
      "I'm boarding a plane in New York to fly to Sydney. The flight is in 10 minutes and the estimated flight\n",
      "First formatted test example:\n",
      "<|im_start|>user\n",
      "<time>2023-09-01T00:00:00</time>\n",
      "How many days passed in August 2023?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's determine how many days of August 2023 have already passed.\n",
      "<think>\n",
      "The current timestamp is September 1, 2023, 00:00:00. This means that all of August has just concluded. The model needs to recall the number of days in August.\n",
      "</think>\n",
      "Today is **September 1, 2023**.\n",
      "<think>\n",
      "August is a month that always has 31 days.\n",
      "</think>\n",
      "Since we are now on September 1st, it means th\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template to both train and test data, do not tokenize yet\n",
    "train_texts = tokenizer.apply_chat_template(\n",
    "    train_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "test_texts = tokenizer.apply_chat_template(\n",
    "    test_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(f\"First formatted train example:\\n{train_texts[0][:500]}\")\n",
    "print(f\"First formatted test example:\\n{test_texts[0][:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba5e67-c4ea-4536-a5c2-24bbdb9bdfbe",
   "metadata": {},
   "source": [
    "### Analyze Sequence Lengths Before Tokenization\n",
    "\n",
    "This cell tokenizes the preformatted prompts (without truncation or special tokens) to estimate sequence lengths.\n",
    "\n",
    "- Reports max, mean, and 90th percentile lengths for both train and test splits.\n",
    "- This helps determine appropriate training sequence length selection strategies later.\n",
    "\n",
    "We do **not yet tokenize for training**‚Äîthis step is purely for exploratory analysis and debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca8acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sequence length: 2057\n",
      "Max test sequence length: 1504\n",
      "Train: Mean = 310.8, 90th percentile = 527\n",
      "Test: Mean = 311.1, 90th percentile = 544\n"
     ]
    }
   ],
   "source": [
    "# Tokenize to find sequence lengths without truncation\n",
    "train_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in train_texts]\n",
    "test_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in test_texts]\n",
    "\n",
    "max_train_len = max(train_lengths)\n",
    "max_test_len = max(test_lengths)\n",
    "\n",
    "print(f\"Max train sequence length: {max_train_len}\")\n",
    "print(f\"Max test sequence length: {max_test_len}\")\n",
    "\n",
    "# Get some stats about the distribution\n",
    "print(f\"Train: Mean = {sum(train_lengths)/len(train_lengths):.1f}, 90th percentile = {sorted(train_lengths)[int(0.9*len(train_lengths))]}\")\n",
    "print(f\"Test: Mean = {sum(test_lengths)/len(test_lengths):.1f}, 90th percentile = {sorted(test_lengths)[int(0.9*len(test_lengths))]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b29255-6f9e-42bf-b787-6740d13e8782",
   "metadata": {},
   "source": [
    "### Convert Texts into TRL-Compatible Format\n",
    "\n",
    "For use with `trl.SFTTrainer`, the data must be wrapped as a list of dictionaries with a single `\"text\"` field.\n",
    "\n",
    "- This format will later be passed into HuggingFace `datasets.Dataset`.\n",
    "- The printed sample verifies that each item has the right structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9cc113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train sample: {'text': \"<|im_start|>system\\nYou are an AI assistant. Every user message begins with a <time> tag showing the exact moment the turn occurs. Sometimes, there's only the timestamp with no text‚Äîthat means time advanced without user input. Use <think>...</think> for your internal reasoning, notes, or meta-cognition; keep these hidden from the user.<|im_end|>\\n<|im_start|>user\\n<time>2023-11-01T10:56:00</time>\\nI'm boarding a plane in New York to fly to Sydney. The flight is in 10 minutes and the estimated flight duration is 21 hours. What's the local time in Sydney when I land?<|im_end|>\\n<|im_start|>assistant\\nLet's calculate your arrival time in Sydney considering the time difference and flight duration.\\n<think>\\nIt's 10:56 AM on November 1st, 2023 in New York.\\n</think>\\nCurrent local time in New York is **10:56 AM**.\\n<think>\\nTakeoff is in 10 minutes, making it 11:06 AM.\\n</think>\\nYou'll be airborne at **11:06 AM** New York time.\\n<think>\\nWith a flight duration of 21 hours, the time of landing in New York time would be 8:06 AM the following day.\\n</think>\\nYou'll land at **8:06 AM** on November 2nd, 2023 in New York time.\\n<think>\\nNew York is currently (in November) 16 hours behind Sydney due to daylight saving time.\\n</think>\\nConvert New York time to Sydney time: \\n<think>\\n8:06 AM in New York translates to 12:06 AM + 1 day in Sydney (November 3rd).\\n</think>\\nSo, your local landing time in Sydney will be **12:06 AM** on November 3rd, 2023.<|im_end|>\\n\"}\n",
      "First eval sample: {'text': \"<|im_start|>user\\n<time>2023-09-01T00:00:00</time>\\nHow many days passed in August 2023?<|im_end|>\\n<|im_start|>assistant\\nLet's determine how many days of August 2023 have already passed.\\n<think>\\nThe current timestamp is September 1, 2023, 00:00:00. This means that all of August has just concluded. The model needs to recall the number of days in August.\\n</think>\\nToday is **September 1, 2023**.\\n<think>\\nAugust is a month that always has 31 days.\\n</think>\\nSince we are now on September 1st, it means the entirety of August has just finished.\\n<think>\\nTherefore, exactly 31 days from August have passed.\\n</think>\\nSo, **31 days** passed in August 2023.<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Format for TRL SFTTrainer: list of dicts with a 'text' field\n",
    "train_dataset = [{\"text\": x} for x in train_texts]\n",
    "eval_dataset = [{\"text\": x} for x in test_texts]\n",
    "\n",
    "print(f\"First train sample: {train_dataset[0]}\")\n",
    "print(f\"First eval sample: {eval_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bff9d1-e6c5-4cd4-b6cf-f343b3fd1eec",
   "metadata": {},
   "source": [
    "### Wrap as HuggingFace Dataset Objects\n",
    "\n",
    "The lists of formatted examples are now converted into `datasets.Dataset` objects to be compatible with `SFTTrainer`.\n",
    "\n",
    "- These objects support batching, shuffling, and streaming.\n",
    "- The printout confirms successful instantiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01cca9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2188\n",
      "}) Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 387\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Wrap the data in HuggingFace Datasets objects\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "eval_dataset = Dataset.from_list(eval_dataset)\n",
    "\n",
    "print(train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd8b8-46af-4a73-935b-a6db652ab8c7",
   "metadata": {},
   "source": [
    "### Load Base Model with `FastLanguageModel`\n",
    "\n",
    "We load the Qwen3-14B base model using **Unsloth's `FastLanguageModel`** with QLoRA-style 4-bit quantization:\n",
    "\n",
    "- `load_in_4bit=True` enables QLoRA (memory-efficient fine-tuning).\n",
    "- `full_finetuning=False` ensures adapter-based training (LoRA).\n",
    "- `max_seq_length` is set to the maximum sequence length seen in the training set (determined earlier).\n",
    "\n",
    "> Note: The `chat_template` must be manually re-injected due to tokenizer reload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487f1034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.8: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Num GPUs = 1. Max memory: 94.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b26ce92fc4415b9ac020123ab16102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_train_len,       # Highest sequence length in train dataset\n",
    "    load_in_4bit = True,                  # QLoRA\n",
    "    load_in_8bit = False,                 # We are using 4-bit\n",
    "    full_finetuning = False,              # QLoRA/PEFT not full FT\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = \"\"\"{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\'}}\\n        {%- if message.role == \"user\" and message.timestamp is defined %}\\n            {{- \\'<time>\\' + message.timestamp + \\'</time>\\\\n\\' }}\\n        {%- endif %}\\n        {{- content + \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\\\n\\' }}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9fd03-89f0-49a2-af12-93ca29f38517",
   "metadata": {},
   "source": [
    "### Inject LoRA Adapters into the Model\n",
    "\n",
    "We apply **LoRA adapters** using Unsloth's wrapper:\n",
    "\n",
    "- Target modules include all major projection layers in MLP and attention blocks (`q_proj`, `v_proj`, `gate_proj`, etc.).\n",
    "- Adapter rank is set to `r=32` with Œ±=32.\n",
    "- Dropout improves generalization but disables full-speed patching in Unsloth.\n",
    "- Gradient checkpointing is used to reduce memory usage at a slight compute cost.\n",
    "\n",
    "> ‚ö†Ô∏è **Note**: We will get a warning from Unsloth which is **benign**. This trade-off was accepted to preserve regularization via dropout. It does not affect correctness or convergence‚Äîonly training speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f7d3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.12.8 patched 40 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,              # Rank of LoRA\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=32,     # Standard\n",
    "    lora_dropout=0.05, # Standard\n",
    "    bias=\"none\",       # Standard\n",
    "    random_state = seed,\n",
    "    use_gradient_checkpointing=True  # Saves memory, a bit slower\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c22bb-74b6-49c5-a1cf-b86f4ab95b56",
   "metadata": {},
   "source": [
    "### Configure and Initialize `SFTTrainer`\n",
    "\n",
    "We use HuggingFace's `SFTTrainer` for instruction tuning:\n",
    "\n",
    "- Batch size = 8 with gradient accumulation of 4 ‚Üí effective batch size = 32.\n",
    "- Optimizer: 8-bit AdamW.\n",
    "- 3 epochs with linear LR schedule, warmup = 100 steps.\n",
    "- Evaluation and logging configured to occur frequently.\n",
    "- All logging is local, and reproducibility is ensured via fixed seed.\n",
    "\n",
    "> This trainer is setup for fine-tuning the model using the Phase 1 single-turn conversations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d10246b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6147bd6590c74816ac5b206eb694ee4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/2188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2974954b85499c9d494d26a4db662a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/387 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,            # Use test set for evaluation\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,    # \n",
    "        gradient_accumulation_steps = 4,    # Effective batch size = 32\n",
    "        warmup_steps = 100,                 # \n",
    "        num_train_epochs = 3,               # Standard\n",
    "        learning_rate = 2e-5,               # Standard\n",
    "        logging_steps = 10,                 # Log every 10 steps\n",
    "        optim = \"adamw_8bit\",               # Standard for QLoRA\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = seed,\n",
    "        report_to = \"none\",                 \n",
    "        max_grad_norm = 1.0,                # Standard\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15088aab-7f5c-454e-ba79-001a5bae49b8",
   "metadata": {},
   "source": [
    "### Inspect GPU Device and Record Initial Memory Usage\n",
    "\n",
    "Before starting training, we record:\n",
    "\n",
    "- The name of the active GPU.\n",
    "- Total available GPU memory.\n",
    "- Reserved memory at the beginning of the run.\n",
    "\n",
    "> This helps contextualize memory usage during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baea60e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Max memory = 94.557 GB.\n",
      "13.334 GB of memory reserved at start.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved at start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012108d-d6a5-4961-9797-b6672f48908a",
   "metadata": {},
   "source": [
    "### Train the Model and Track Peak GPU Memory Usage\n",
    "\n",
    "The actual fine-tuning begins using `trainer.train()`.\n",
    "\n",
    "- After completion, we log the **peak GPU memory** reserved during the training process (in GB).\n",
    "- This value reflects memory requirements under 4-bit QLoRA with checkpointing.\n",
    "\n",
    "Training stats (loss, etc.) are stored in `trainer_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9511b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2,188 | Num Epochs = 3 | Total steps = 207\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 19:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.780300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.641700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.550200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.543800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory used during training: 17.695 GB.\n"
     ]
    }
   ],
   "source": [
    "# Begin training, collect stats\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# After training, report peak memory reserved\n",
    "peak_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak GPU memory used during training: {peak_gpu_memory} GB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbdddf-1035-4f4c-bcc7-e98d7ea252d3",
   "metadata": {},
   "source": [
    "### Evaluate the Fine-Tuned Model on Held-Out Phase 1 Test Set\n",
    "\n",
    "To assess the model after training:\n",
    "\n",
    "- We switch to `gradient_accumulation_steps = 1` for evaluation.\n",
    "- `trainer.evaluate()` is run on the Phase 1 test data.\n",
    "\n",
    "Results include loss and other metrics; these are printed for transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daade0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [97/97 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.5381345152854919, 'eval_runtime': 26.1575, 'eval_samples_per_second': 14.795, 'eval_steps_per_second': 3.708, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.args.gradient_accumulation_steps = 1\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Eval metrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7392b1d-c84d-473a-99d0-a171d1521786",
   "metadata": {},
   "source": [
    "### Save LoRA Adapter and Tokenizer After Training\n",
    "\n",
    "After training, we save:\n",
    "\n",
    "- The **LoRA adapter** weights using `model.save_pretrained`.\n",
    "- The tokenizer configuration using `tokenizer.save_pretrained`.\n",
    "\n",
    "> These saved adapters are necessary to resume or merge LoRA layers later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2190ecc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-14Bphase1_adapter/tokenizer_config.json',\n",
       " 'Qwen/Qwen3-14Bphase1_adapter/special_tokens_map.json',\n",
       " 'Qwen/Qwen3-14Bphase1_adapter/chat_template.jinja',\n",
       " 'Qwen/Qwen3-14Bphase1_adapter/vocab.json',\n",
       " 'Qwen/Qwen3-14Bphase1_adapter/merges.txt',\n",
       " 'Qwen/Qwen3-14Bphase1_adapter/added_tokens.json',\n",
       " 'Qwen/Qwen3-14Bphase1_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_name+\"phase1_adapter\")\n",
    "tokenizer.save_pretrained(model_name+\"phase1_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8007da6-51ee-49d8-8d13-04c9edabeb40",
   "metadata": {},
   "source": [
    "### Free GPU and Host Memory (Intermediate Cleanup)\n",
    "\n",
    "We delete the training artifacts:\n",
    "\n",
    "- `model`, `tokenizer`, and `trainer` are explicitly deleted.\n",
    "- Python's garbage collector is triggered.\n",
    "- GPU memory is cleared using `torch.cuda.empty_cache()`.\n",
    "\n",
    "> This prepares the environment for the final merge step without exceeding memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70de1ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae59885-bc85-4539-bfa7-1bcde1fee050",
   "metadata": {},
   "source": [
    "### Reload Base Model and Phase 1 Tokenizer (for Merging)\n",
    "\n",
    "We now reload:\n",
    "\n",
    "- The **original base model** (Qwen3-14B) in `bfloat16` for full-precision merge.\n",
    "- The **tokenizer** saved with the LoRA adapter.\n",
    "\n",
    "> This prepares us to merge adapter weights into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab110a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d33bbd3e8a40e0a1110d9a5e48f067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase1_adapter\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2003fb9f-44dd-4eb3-9429-5f5d5080d29c",
   "metadata": {},
   "source": [
    "### Merge Adapter Weights and Save Final Phase 1 Model\n",
    "\n",
    "This step:\n",
    "\n",
    "- Wraps the base model with PEFT's `PeftModel` and loads the adapter.\n",
    "- Merges LoRA weights into the base model (`merge_and_unload()`).\n",
    "- Saves the **final merged model** and tokenizer to `\"phase1\"` directory.\n",
    "\n",
    "> This produces a single checkpoint with integrated parameters for downstream training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24a72ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-14Bphase1/tokenizer_config.json',\n",
       " 'Qwen/Qwen3-14Bphase1/special_tokens_map.json',\n",
       " 'Qwen/Qwen3-14Bphase1/chat_template.jinja',\n",
       " 'Qwen/Qwen3-14Bphase1/vocab.json',\n",
       " 'Qwen/Qwen3-14Bphase1/merges.txt',\n",
       " 'Qwen/Qwen3-14Bphase1/added_tokens.json',\n",
       " 'Qwen/Qwen3-14Bphase1/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_name+\"phase1_adapter\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(model_name+\"phase1\")\n",
    "tokenizer.save_pretrained(model_name+\"phase1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfdd5e-1c71-4812-817f-dc71ad89bc86",
   "metadata": {},
   "source": [
    "### Final Cleanup\n",
    "\n",
    "We delete the model and tokenizer objects again to free memory before proceeding to the next phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40585f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a19aba-a2d5-450a-83ec-05fb61b78b18",
   "metadata": {},
   "source": [
    "# Phase 2: Two-Turn Temporal Scenarios\n",
    "\n",
    "Phase 2 curriculum introduces **structured two-turn conversations**, primarily designed to develop **temporal awareness** across contexts. These examples encourage the model to reason about elapsed time, memory continuity, and changes in user intent‚Äîacross two turns.\n",
    "\n",
    "‚ö†Ô∏è **Implementation Note**: The training code and workflow are structurally identical to Phase 1. All notebook cells are reused with minor changes like:\n",
    "\n",
    "- `load_split('phase2_train.json', 'phase2_test.json')`\n",
    "- Final model saved as `\"Qwen/Qwen3-14Bphase2\"`\n",
    "\n",
    "If Phase 1 has already been run, you can begin directly by executing the notebook cells of Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f21d00f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5291 train and 935 test conversations.\n"
     ]
    }
   ],
   "source": [
    "# Load phase 2 data\n",
    "train_data, test_data = load_split('phase2_train.json', 'phase2_test.json')\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train and {len(test_data)} test conversations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30ebe4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase1\")\n",
    "tokenizer.chat_template = \"\"\"{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\'}}\\n        {%- if message.role == \"user\" and message.timestamp is defined %}\\n            {{- \\'<time>\\' + message.timestamp + \\'</time>\\\\n\\' }}\\n        {%- endif %}\\n        {{- content + \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\\\n\\' }}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71152461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First formatted train example:\n",
      "<|im_start|>user\n",
      "<time>2024-05-10T16:19:14</time>\n",
      "I'm running an experiment that finishes at 5 PM, and I need to understand the role of catalysts in chemical reactions. Can you explain how they work as I monitor the results?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Absolutely, let's cover catalysts while you oversee your experiment.\n",
      "<think>\n",
      "The experiment concludes at 5 PM, so there‚Äôs about 40 minutes available for this explanation. I‚Äôll focus on the essentials and practical examples.\n",
      "</think>\n",
      "A **cataly\n",
      "First formatted test example:\n",
      "<|im_start|>user\n",
      "<time>2023-07-25T18:00:00</time>\n",
      "My friend is flying from London to New York. The flight departs London at 10:00 AM local time on July 26th and the flight duration is 7 hours. New York is 5 hours behind London. What time should I expect them to land in New York?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's calculate your friend's arrival time in New York, accounting for flight duration and the time difference.\n",
      "<think>\n",
      "The user wants to know the arrival time in New York. Current time is\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template to both train and test data, do not tokenize yet\n",
    "train_texts = tokenizer.apply_chat_template(\n",
    "    train_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "test_texts = tokenizer.apply_chat_template(\n",
    "    test_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(f\"First formatted train example:\\n{train_texts[0][:500]}\")\n",
    "print(f\"First formatted test example:\\n{test_texts[0][:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91e3b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sequence length: 3795\n",
      "Max test sequence length: 2548\n",
      "Train: Mean = 483.4, 90th percentile = 904\n",
      "Test: Mean = 490.3, 90th percentile = 903\n"
     ]
    }
   ],
   "source": [
    "# Tokenize to find sequence lengths without truncation\n",
    "train_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in train_texts]\n",
    "test_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in test_texts]\n",
    "\n",
    "max_train_len = max(train_lengths)\n",
    "max_test_len = max(test_lengths)\n",
    "\n",
    "print(f\"Max train sequence length: {max_train_len}\")\n",
    "print(f\"Max test sequence length: {max_test_len}\")\n",
    "\n",
    "# Get some stats about the distribution\n",
    "print(f\"Train: Mean = {sum(train_lengths)/len(train_lengths):.1f}, 90th percentile = {sorted(train_lengths)[int(0.9*len(train_lengths))]}\")\n",
    "print(f\"Test: Mean = {sum(test_lengths)/len(test_lengths):.1f}, 90th percentile = {sorted(test_lengths)[int(0.9*len(test_lengths))]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2f8fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train sample: {'text': \"<|im_start|>user\\n<time>2024-05-10T16:19:14</time>\\nI'm running an experiment that finishes at 5 PM, and I need to understand the role of catalysts in chemical reactions. Can you explain how they work as I monitor the results?<|im_end|>\\n<|im_start|>assistant\\nAbsolutely, let's cover catalysts while you oversee your experiment.\\n<think>\\nThe experiment concludes at 5 PM, so there‚Äôs about 40 minutes available for this explanation. I‚Äôll focus on the essentials and practical examples.\\n</think>\\nA **catalyst** is a substance that speeds up a chemical reaction without being consumed in the process. Here‚Äôs how it works:\\n\\n- **Lower Activation Energy**: Catalysts provide an alternative pathway for the reaction with a lower activation energy. This means that more collisions between reacting molecules have enough energy to overcome this barrier, increasing the reaction rate.\\n- **Reversible**: Catalysts are not permanently altered by the reaction. They participate in the reaction cycle and are regenerated at the end, ready to assist another cycle.\\n\\n### Types of Catalysts:\\n- **Homogeneous Catalysts**: These are in the same phase as the reactants. For example, an acid catalyst in an aqueous solution.\\n- **Heterogeneous Catalysts**: These are in a different phase than the reactants, often solid catalysts in liquid or gas reactants.\\n\\n### Real-World Examples:\\n- **Enzymes**: Biological catalysts that speed up biochemical reactions in cells. An important example is the enzyme *catalase*, which breaks down hydrogen peroxide into water and oxygen, protecting cells from oxidative damage.\\n- **Industrial Catalysts**: In the production of ammonia (Haber process), iron acts as a catalyst to combine nitrogen and hydrogen gases efficiently.\\n\\n<think>\\nI should remind them of the catalyst‚Äôs role in their experiment and offer any further assistance.\\n</think>\\nIn your experiment, if you‚Äôre using a catalyst, observe how it accelerates the reaction compared to a control setup without it. If you have any more questions as you collect your data, feel free to ask!<|im_end|>\\n<|im_start|>user\\n<time>2024-05-10T17:05:32</time>\\nI just wrapped up the experiment ‚Äî the catalyst worked exactly as you described. Thanks for the detailed explanation. What should I focus on for a follow-up study?<|im_end|>\\n<|im_start|>assistant\\nGreat to hear your experiment went well! \\n<think>\\nThe experiment finished at 5 PM, and it's now 5:05:32 PM. The user is thinking about future directions. I should provide guidance on extending the research with insightful angles.\\n</think>\\nFor a follow-up study, consider these angles:\\n\\n1. **Catalyst Efficiency**: Quantify how much the catalyst reduces the activation energy and increases the reaction rate. Compare different catalysts for the same reaction to identify which is most efficient.\\n2. **Catalyst Stability and Reusability**: Investigate how many cycles the catalyst can undergo before losing effectiveness. This can be crucial for industrial applications where cost-effectiveness matters.\\n3. **Reaction Mechanism**: Delve into the molecular interactions the catalyst facilitates. Understanding how the catalyst achieves its effect can lead to innovations in catalyst design.\\n4. **Environmental Impact**: Explore green chemistry aspects, such as developing catalysts that minimize waste or work under milder conditions.\\n5. **Scale-Up Studies**: If the reaction has industrial relevance, investigate how the catalyst performs in larger-scale operations to assess scalability.\\n\\nEach of these paths can yield valuable insights and potentially pave the way for innovative applications. If you want to explore any of these topics in detail or need data analysis support, I'm here to help!<|im_end|>\\n\"}\n",
      "First eval sample: {'text': \"<|im_start|>user\\n<time>2023-07-25T18:00:00</time>\\nMy friend is flying from London to New York. The flight departs London at 10:00 AM local time on July 26th and the flight duration is 7 hours. New York is 5 hours behind London. What time should I expect them to land in New York?<|im_end|>\\n<|im_start|>assistant\\nLet's calculate your friend's arrival time in New York, accounting for flight duration and the time difference.\\n<think>\\nThe user wants to know the arrival time in New York. Current time is 6:00:00 PM on July 25th, 2023. The flight departs London at 10:00 AM on July 26th. Flight duration is 7 hours. New York is 5 hours behind London.\\nStep 1: Calculate arrival time in London local time.\\nDeparture London: July 26th, 10:00 AM.\\nFlight duration: 7 hours.\\nArrival London local time: July 26th, 10:00 AM + 7 hours = July 26th, 5:00 PM.\\nStep 2: Convert London arrival time to New York time.\\nNew York is 5 hours behind London.\\nJuly 26th, 5:00 PM (London time) - 5 hours = July 26th, 12:00 PM (New York time).\\n</think>\\nYour friend's flight departs London at **10:00 AM on July 26th**.\\n\\nAdding the **7-hour flight duration**, they would land in London at **5:00 PM London time** on July 26th.\\n\\nNow, accounting for the **5-hour time difference** (New York being behind London), we subtract 5 hours from the London arrival time.\\n\\nSo, your friend should be expected to land in New York at **12:00 PM (noon) on July 26th**.<|im_end|>\\n<|im_start|>user\\n<time>2023-07-26T12:15:00</time>\\nThey just texted me! They said they landed 20 minutes ago. Was my calculation off, or was their flight early?<|im_end|>\\n<|im_start|>assistant\\nThat's great news they landed! Let's see if their early arrival lines up with our previous estimate.\\n<think>\\nThe user's friend landed 20 minutes ago. The current timestamp is 12:15:00 PM on July 26th, 2023. This means the friend landed at 12:15 PM - 20 minutes = 11:55 AM.\\nThe previously calculated expected arrival time was 12:00 PM (noon) on July 26th.\\nCompare actual landing time (11:55 AM) with expected landing time (12:00 PM).\\n</think>\\nYour friend texted you at **12:15 PM**, saying they landed 20 minutes ago. This means their actual landing time was **11:55 AM**.\\n\\nOur previous calculation estimated an arrival time of **12:00 PM (noon)**.\\n\\nSince they landed at **11:55 AM**, which is 5 minutes before the estimated **12:00 PM** arrival, their flight was indeed **5 minutes early**! Your calculation was spot on, the flight just made good time.<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Format for TRL SFTTrainer: list of dicts with a 'text' field\n",
    "train_dataset = [{\"text\": x} for x in train_texts]\n",
    "eval_dataset = [{\"text\": x} for x in test_texts]\n",
    "\n",
    "print(f\"First train sample: {train_dataset[0]}\")\n",
    "print(f\"First eval sample: {eval_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d688e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 5291\n",
      "}) Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 935\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Wrap the data in HuggingFace Datasets objects\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "eval_dataset = Dataset.from_list(eval_dataset)\n",
    "\n",
    "print(train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a1b4764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.8: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Num GPUs = 1. Max memory: 94.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d271d7919c4a4ce9964a8adf40d31ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name+\"phase1\",\n",
    "    max_seq_length = max_train_len,       # Highest sequence length in train dataset\n",
    "    load_in_4bit = True,                  # QLoRA\n",
    "    load_in_8bit = False,                 # We are using 4-bit\n",
    "    full_finetuning = False,              # QLoRA/PEFT not full FT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bca0b06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.12.8 patched 40 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,              # Rank of LoRA\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=32,     # Standard\n",
    "    lora_dropout=0.05, # Standard\n",
    "    bias=\"none\",       # Standard\n",
    "    random_state = seed,\n",
    "    use_gradient_checkpointing=True  # Saves memory, a bit slower\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "846ebc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2bb005f93e45f9b70150bff7507103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/5291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadf766c94354ee881178c2552e1b767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/935 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,            # Use test set for evaluation\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,    # \n",
    "        gradient_accumulation_steps = 4,    # Effective batch size = 32\n",
    "        warmup_steps = 100,                 # \n",
    "        num_train_epochs = 3,               # Standard\n",
    "        learning_rate = 2e-5,               # Standard\n",
    "        logging_steps = 10,                 # Log every 10 steps\n",
    "        optim = \"adamw_8bit\",               # Standard for QLoRA\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = seed,\n",
    "        report_to = \"none\",                 \n",
    "        max_grad_norm = 1.0,                # Standard\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0794974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Max memory = 94.557 GB.\n",
      "13.328 GB of memory reserved at start.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved at start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5d35fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,291 | Num Epochs = 3 | Total steps = 498\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='498' max='498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [498/498 1:09:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.975300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.830300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.792800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.816200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.742300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.715800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.721600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory used during training: 23.104 GB.\n"
     ]
    }
   ],
   "source": [
    "# Begin training, collect stats\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# After training, report peak memory reserved\n",
    "peak_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak GPU memory used during training: {peak_gpu_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1c3be5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 01:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.7074225544929504, 'eval_runtime': 91.4907, 'eval_samples_per_second': 10.22, 'eval_steps_per_second': 2.558, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.args.gradient_accumulation_steps = 1\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Eval metrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53fa17a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-14Bphase2_adapter/tokenizer_config.json',\n",
       " 'Qwen/Qwen3-14Bphase2_adapter/special_tokens_map.json',\n",
       " 'Qwen/Qwen3-14Bphase2_adapter/chat_template.jinja',\n",
       " 'Qwen/Qwen3-14Bphase2_adapter/vocab.json',\n",
       " 'Qwen/Qwen3-14Bphase2_adapter/merges.txt',\n",
       " 'Qwen/Qwen3-14Bphase2_adapter/added_tokens.json',\n",
       " 'Qwen/Qwen3-14Bphase2_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_name+\"phase2_adapter\")\n",
    "tokenizer.save_pretrained(model_name+\"phase2_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b0a1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "967f16a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961034bf354047aea08bbd323bd1dce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase2_adapter\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name+\"phase1\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce7f0104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-14Bphase2/tokenizer_config.json',\n",
       " 'Qwen/Qwen3-14Bphase2/special_tokens_map.json',\n",
       " 'Qwen/Qwen3-14Bphase2/chat_template.jinja',\n",
       " 'Qwen/Qwen3-14Bphase2/vocab.json',\n",
       " 'Qwen/Qwen3-14Bphase2/merges.txt',\n",
       " 'Qwen/Qwen3-14Bphase2/added_tokens.json',\n",
       " 'Qwen/Qwen3-14Bphase2/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_name+\"phase2_adapter\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(model_name+\"phase2\")\n",
    "tokenizer.save_pretrained(model_name+\"phase2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c897da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a0d25-c26d-418c-b314-2de7ee3fcc7e",
   "metadata": {},
   "source": [
    "# Phase 3: Multi-Turn Generalization and Reasoning Modulation\n",
    "\n",
    "Phase 3 expands to **three-turn and longer conversations**, generalizing beyond strictly temporal scenarios.\n",
    "\n",
    "‚ö†Ô∏è **Implementation Note**: Training infrastructure remains the same with minor changes like:\n",
    "\n",
    "- `load_split('phase3_train.json', 'phase3_test.json')`\n",
    "- Save path updated to `\"Qwen/Qwen3-14Bphase3\"`\n",
    "\n",
    "Phase 3 can be run independently as long as prior phases have generated the expected model weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c8c4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5878 train and 1039 test conversations.\n"
     ]
    }
   ],
   "source": [
    "# Load phase 3 data\n",
    "train_data, test_data = load_split('phase3_train.json', 'phase3_test.json')\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train and {len(test_data)} test conversations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e7473d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase2\")\n",
    "tokenizer.chat_template = \"\"\"{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\'}}\\n        {%- if message.role == \"user\" and message.timestamp is defined %}\\n            {{- \\'<time>\\' + message.timestamp + \\'</time>\\\\n\\' }}\\n        {%- endif %}\\n        {{- content + \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\\\n\\' }}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c525c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First formatted train example:\n",
      "<|im_start|>user\n",
      "<time>2023-06-14T09:00:00</time>\n",
      "I‚Äôm about to start a 10-hour work marathon. How can I stay productive?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A long day ahead! Let's maximize your productivity.\n",
      "\n",
      "<think>\n",
      "9:00 AM start ‚Äî 10 hours means they‚Äôll finish by 7:00 PM. Solid block.\n",
      "</think>\n",
      "\n",
      "Suggestions:\n",
      "- **Breaks:** Take a 5-minute break every hour. Recharge often!\n",
      "- **Hydration:** Keep water at your desk.\n",
      "- **Prioritize tasks:** List critical items first.\n",
      "- **Stretch:** Every couple of hour\n",
      "First formatted test example:\n",
      "<|im_start|>user\n",
      "<time>2023-03-15T09:37:18</time>\n",
      "I just got promoted to team lead! What should I focus on in my first 30 days?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Congratulations on your promotion! As you step into this new role, here are some key areas to focus on during your first 30 days:\n",
      "\n",
      "- **Understand team dynamics**: Spend time getting to know each team member's strengths and areas for improvement.\n",
      "- **Set clear goals**: Align with management on short-term and long-term objectives.\n",
      "- **Estab\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template to both train and test data, do not tokenize yet\n",
    "train_texts = tokenizer.apply_chat_template(\n",
    "    train_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "test_texts = tokenizer.apply_chat_template(\n",
    "    test_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(f\"First formatted train example:\\n{train_texts[0][:500]}\")\n",
    "print(f\"First formatted test example:\\n{test_texts[0][:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50684027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sequence length: 3795\n",
      "Max test sequence length: 3041\n",
      "Train: Mean = 496.0, 90th percentile = 855\n",
      "Test: Mean = 464.7, 90th percentile = 752\n"
     ]
    }
   ],
   "source": [
    "# Tokenize to find sequence lengths without truncation\n",
    "train_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in train_texts]\n",
    "test_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in test_texts]\n",
    "\n",
    "max_train_len = max(train_lengths)\n",
    "max_test_len = max(test_lengths)\n",
    "\n",
    "print(f\"Max train sequence length: {max_train_len}\")\n",
    "print(f\"Max test sequence length: {max_test_len}\")\n",
    "\n",
    "# Get some stats about the distribution\n",
    "print(f\"Train: Mean = {sum(train_lengths)/len(train_lengths):.1f}, 90th percentile = {sorted(train_lengths)[int(0.9*len(train_lengths))]}\")\n",
    "print(f\"Test: Mean = {sum(test_lengths)/len(test_lengths):.1f}, 90th percentile = {sorted(test_lengths)[int(0.9*len(test_lengths))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a94506e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train sample: {'text': \"<|im_start|>user\\n<time>2023-06-14T09:00:00</time>\\nI‚Äôm about to start a 10-hour work marathon. How can I stay productive?<|im_end|>\\n<|im_start|>assistant\\nA long day ahead! Let's maximize your productivity.\\n\\n<think>\\n9:00 AM start ‚Äî 10 hours means they‚Äôll finish by 7:00 PM. Solid block.\\n</think>\\n\\nSuggestions:\\n- **Breaks:** Take a 5-minute break every hour. Recharge often!\\n- **Hydration:** Keep water at your desk.\\n- **Prioritize tasks:** List critical items first.\\n- **Stretch:** Every couple of hours, move around.\\n- **Healthy snacks:** Keep energy levels stable.\\n\\nYou‚Äôve got this!<|im_end|>\\n<|im_start|>user\\n<time>2023-06-14T19:00:00</time>\\nI just wrapped up. Any tips on how to unwind after such a day?<|im_end|>\\n<|im_start|>assistant\\nCongrats on finishing!\\n\\n<think>\\n7:00 PM ‚Äî the user completed the marathon. They need effective unwinding.\\n</think>\\n\\nSuggestions:\\n- **Decompress:** A warm shower to relax muscles.\\n- **Light meal:** Replenish energy with something nutritious.\\n- **Mindfulness/meditation:** Calm the mind.\\n- **Gentle activity:** A walk or light yoga.\\n- **Disconnect:** Avoid screens for a bit.\\n\\nRest well, you earned it!<|im_end|>\\n<|im_start|>user\\n<time>2023-06-15T02:30:00</time>\\nI can‚Äôt sleep. My mind is still racing from all the work. What should I do?<|im_end|>\\n<|im_start|>assistant\\nIt‚Äôs tough when your mind doesn‚Äôt switch off.\\n\\n<think>\\nIt‚Äôs 2:30 AM ‚Äî hours after they planned to rest. Their mind is still active.\\n</think>\\n\\nHere‚Äôs what might help:\\n- **Breathing exercises:** Deep, slow breaths to calm your system.\\n- **Write it out:** Jot down thoughts to clear your mind.\\n- **Ambient sounds:** Soft music or nature sounds can soothe.\\n- **Read:** A calming book, avoid screens.\\n\\n<think>\\nThey need to ease the mind gently, without pressure.\\n</think>\\n\\nIf it persists, don‚Äôt stress ‚Äî rest will come. Be kind to yourself.<|im_end|>\\n\"}\n",
      "First eval sample: {'text': \"<|im_start|>user\\n<time>2023-03-15T09:37:18</time>\\nI just got promoted to team lead! What should I focus on in my first 30 days?<|im_end|>\\n<|im_start|>assistant\\nCongratulations on your promotion! As you step into this new role, here are some key areas to focus on during your first 30 days:\\n\\n- **Understand team dynamics**: Spend time getting to know each team member's strengths and areas for improvement.\\n- **Set clear goals**: Align with management on short-term and long-term objectives.\\n- **Establish communication**: Hold regular check-ins to foster open dialogue.\\n- **Identify quick wins**: Look for small projects or improvements that can boost team morale and demonstrate your leadership.\\n\\n<think>\\nThe user is transitioning into a leadership role, so guiding them on relationship-building and strategic planning is essential.\\n</think>\\n\\nYou're on an exciting path‚Äîembrace the challenges and opportunities ahead!<|im_end|>\\n<|im_start|>user\\n<time>2023-03-30T10:45:52</time>\\nI've started implementing some changes and the feedback is mixed. How should I handle this?<|im_end|>\\n<|im_start|>assistant\\nMixed feedback is a valuable opportunity to learn and adjust. Here‚Äôs how to navigate it:\\n\\n- **Listen actively**: Encourage open feedback sessions, and listen without interrupting.\\n- **Reflect and adapt**: Identify common themes in the feedback and consider adjustments.\\n- **Communicate changes**: Clearly explain any decisions or changes to the team based on their input.\\n- **Stay positive and patient**: Change takes time, and consistent communication will help ease transitions.\\n\\n<think>\\nThe user is in the early stages of their leadership role, and it‚Äôs normal for adjustments to receive varied reactions.\\n</think>\\n\\nYou‚Äôre already showing strong leadership by engaging with feedback‚Äîkeep going!<|im_end|>\\n<|im_start|>user\\n<time>2024-06-15T15:22:47</time>\\nI‚Äôve been asked to lead the entire division. It feels overwhelming. What should my first steps be?<|im_end|>\\n<|im_start|>assistant\\nBeing entrusted with an entire division is a significant milestone. Here‚Äôs how to approach it:\\n\\n<think>\\nOver a year has passed since the user‚Äôs first promotion, indicating substantial growth and trust from their organization.\\n</think>\\n\\n- **Review the strategic vision**: Ensure alignment with organizational goals.\\n- **Meet with key stakeholders**: Understand their expectations and challenges.\\n- **Assess resources and capabilities**: Evaluate the strengths and needs of each team under your division.\\n- **Create a transition plan**: Outline immediate priorities and a roadmap for the division.\\n\\n<think>\\nWith their prior leadership experience, they have a foundation to build upon. This is their opportunity to extend their influence and vision.\\n</think>\\n\\nCongratulations on this new chapter‚Äîyour leadership journey continues to evolve!<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Format for TRL SFTTrainer: list of dicts with a 'text' field\n",
    "train_dataset = [{\"text\": x} for x in train_texts]\n",
    "eval_dataset = [{\"text\": x} for x in test_texts]\n",
    "\n",
    "print(f\"First train sample: {train_dataset[0]}\")\n",
    "print(f\"First eval sample: {eval_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b8d463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 5878\n",
      "}) Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1039\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Wrap the data in HuggingFace Datasets objects\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "eval_dataset = Dataset.from_list(eval_dataset)\n",
    "\n",
    "print(train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a8b5926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.8: Fast Qwen3 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Num GPUs = 1. Max memory: 94.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5bb750b08841fc9ebaf9a42a269f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name+\"phase2\",\n",
    "    max_seq_length = max_train_len,       # Highest sequence length in train dataset\n",
    "    load_in_4bit = True,                  # QLoRA\n",
    "    load_in_8bit = False,                 # We are using 4-bit\n",
    "    full_finetuning = False,              # QLoRA/PEFT not full FT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e133a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,              # Rank of LoRA\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=32,     # Standard\n",
    "    lora_dropout=0.05, # Standard\n",
    "    bias=\"none\",       # Standard\n",
    "    random_state = seed,\n",
    "    use_gradient_checkpointing=True  # Saves memory, a bit slower\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9409c68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2797abdbc444dea433a853ecb5183a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/5878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31569f1a71a455782a28c109297057e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/1039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,            # Use test set for evaluation\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,    # \n",
    "        gradient_accumulation_steps = 4,    # Effective batch size = 32\n",
    "        warmup_steps = 100,                 # \n",
    "        num_train_epochs = 3,               # Standard\n",
    "        learning_rate = 2e-5,               # Standard\n",
    "        logging_steps = 10,                 # Log every 10 steps\n",
    "        optim = \"adamw_8bit\",               # Standard for QLoRA\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = seed,\n",
    "        report_to = \"none\",                 \n",
    "        max_grad_norm = 1.0,                # Standard\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53faed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Max memory = 94.557 GB.\n",
      "23.104 GB of memory reserved at start.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved at start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21890331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,878 | Num Epochs = 3 | Total steps = 552\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='552' max='552' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [552/552 1:24:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.893300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.868400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.808500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.800900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.803700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.771100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.767700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.732500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.740400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.753400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.746400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.726100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.741400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory used during training: 33.475 GB.\n"
     ]
    }
   ],
   "source": [
    "# Begin training, collect stats\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# After training, report peak memory reserved\n",
    "peak_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak GPU memory used during training: {peak_gpu_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab853800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 01:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.7413343787193298, 'eval_runtime': 96.4224, 'eval_samples_per_second': 10.776, 'eval_steps_per_second': 2.696, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.args.gradient_accumulation_steps = 1\n",
    "eval_metrics = trainer.evaluate()\n",
    "print(\"Eval metrics:\", eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fe45e-d99d-436e-afae-326905b12c56",
   "metadata": {},
   "source": [
    "### Manual Sanity Check: Inference with Chat Template\n",
    "\n",
    "This cell performs a quick **manual test** to ensure:\n",
    "\n",
    "1. The **chat template** is correctly applied in inference.\n",
    "2. The **trained model** can produce a sensible response using a realistic prompt and timestamp.\n",
    "\n",
    "We use a simple date-based query (‚ÄúHow many days till Christmas?‚Äù) with a fresh ISO 8601 timestamp injected. The prompt is tokenized using the same chat template applied during training to confirm consistency between training and inference pipelines.\n",
    "\n",
    "‚ö†Ô∏è This is not part of evaluation, just a live check that tokenizer formatting, device placement, and sampling are functioning as expected after training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95524a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "<time>2025-12-22T09:05:23</time>\n",
      "How many days till Christmas?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's calculate the number of days until Christmas.\n",
      "\n",
      "### Current Date and Target Date\n",
      "- **Today's Date:** December 22, 2025\n",
      "- **Christmas Date:** December 25, 2025\n",
      "\n",
      "<think>\n",
      "The user is asking about the days between December 22 and December 25, 2025.\n",
      "</think>\n",
      "\n",
      "### Calculation\n",
      "- **December 22 to December 23:** 1 day\n",
      "- **December 23 to December 24:** 1 day\n",
      "- **December 24 to December 25:** 1 day\n",
      "\n",
      "<think>\n",
      "Summing up the days: 1 + 1 + 1 = 3 days.\n",
      "</think>\n",
      "\n",
      "There are **3 days** left until Christmas.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"How many days till Christmas?\", \"timestamp\": datetime.now().isoformat()[:19]}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = False,\n",
    ")\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256, # Increase for longer outputs!\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3438075a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-14Bphase3_adapter/tokenizer_config.json',\n",
       " 'Qwen/Qwen3-14Bphase3_adapter/special_tokens_map.json',\n",
       " 'Qwen/Qwen3-14Bphase3_adapter/chat_template.jinja',\n",
       " 'Qwen/Qwen3-14Bphase3_adapter/vocab.json',\n",
       " 'Qwen/Qwen3-14Bphase3_adapter/merges.txt',\n",
       " 'Qwen/Qwen3-14Bphase3_adapter/added_tokens.json',\n",
       " 'Qwen/Qwen3-14Bphase3_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_name+\"phase3_adapter\")\n",
    "tokenizer.save_pretrained(model_name+\"phase3_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84af6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6e2fb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2433e96788334939956e82682667791f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase3_adapter\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name+\"phase2\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74409bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Qwen/Qwen3-14Bphase3/tokenizer_config.json',\n",
       " 'Qwen/Qwen3-14Bphase3/special_tokens_map.json',\n",
       " 'Qwen/Qwen3-14Bphase3/chat_template.jinja',\n",
       " 'Qwen/Qwen3-14Bphase3/vocab.json',\n",
       " 'Qwen/Qwen3-14Bphase3/merges.txt',\n",
       " 'Qwen/Qwen3-14Bphase3/added_tokens.json',\n",
       " 'Qwen/Qwen3-14Bphase3/tokenizer.json')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_name+\"phase3_adapter\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(model_name+\"phase3\")\n",
    "tokenizer.save_pretrained(model_name+\"phase3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbc4f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6912ac-eb02-4d64-9fe0-2962a606de73",
   "metadata": {},
   "source": [
    "# Phase 4: Maximal Diversity, Full-Batch Alignment\n",
    "\n",
    "This phase implements the decisive **gradient-aligned convergence** step using a **hand-curated, ultra-diverse batch** of just 128 conversations‚Äîeach structurally and stylistically distinct, unified only by their reliance on temporal and contextual meta-reasoning.\n",
    "\n",
    "Key differences from earlier phases:\n",
    "\n",
    "- **No test split**: all 128 samples are for alignment (pure SFT, no replay, no validation set).\n",
    "- **Model initialization**: load the *Phase 3* checkpoint as the base.\n",
    "- **High learning rate** (`1.5e-4`) with **large effective batch size of 128** and **very few steps**.\n",
    "- **Trainer config**: single step per epoch with effective batch size of 128 (`grad_accumulation_steps=64`, batch size = 2) so each step processes 128 examples (8 √ó 16 accumulations) with no sampling so batch structure remains deterministic.\n",
    "- **Save every epoch**: Each checkpoint is saved for later manual behavioral selection, since degenerate collapse can occur beyond a threshold.\n",
    "\n",
    "This phase is where **all structural entropy is injected**‚Äîcontradictory scenarios, formats, and tones ensure that the **only consistently learnable signal is temporal, meta-cognitive alignment**. The workflow otherwise mirrors previous phases, but with these critical modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a551778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128 train conversations.\n"
     ]
    }
   ],
   "source": [
    "# Load phase 4 data\n",
    "with open('phase4.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} train conversations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef660e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase3\")\n",
    "tokenizer.chat_template = \"\"\"{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for message in messages[::-1] %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(\\'<tool_response>\\') and message.content.endswith(\\'</tool_response>\\')) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if message.content is string %}\\n        {%- set content = message.content %}\\n    {%- else %}\\n        {%- set content = \\'\\' %}\\n    {%- endif %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\'}}\\n        {%- if message.role == \"user\" and message.timestamp is defined %}\\n            {{- \\'<time>\\' + message.timestamp + \\'</time>\\\\n\\' }}\\n        {%- endif %}\\n        {{- content + \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\\\n\\' }}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56cb0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First formatted train example:\n",
      "<|im_start|>user\n",
      "<time>2025-06-08T03:45:13</time>\n",
      "pls fast how do i convert like a 60 page word doc to latex latex i need latex i have to submit in the morning plz help<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I can help you convert your Word document to LaTeX quickly! Don‚Äôt panic‚Äîyou can do this, even at 3:45 AM.\n",
      "<think>\n",
      "It's very late at night (3:45 AM) and the user is clearly stressed and facing a tight deadline. Their grammar and typing suggest panic. This is likely a thesis or major assignment. My j\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template to training data, do not tokenize yet\n",
    "train_texts = tokenizer.apply_chat_template(\n",
    "    train_data,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(f\"First formatted train example:\\n{train_texts[0][:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "612b18da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sequence length: 1379\n",
      "Train: Mean = 735.8, 90th percentile = 981\n"
     ]
    }
   ],
   "source": [
    "# Tokenize to find sequence lengths without truncation\n",
    "train_lengths = [len(tokenizer(t, add_special_tokens=False)['input_ids']) for t in train_texts]\n",
    "\n",
    "max_train_len = max(train_lengths)\n",
    "\n",
    "print(f\"Max train sequence length: {max_train_len}\")\n",
    "\n",
    "# Get some stats about the distribution\n",
    "print(f\"Train: Mean = {sum(train_lengths)/len(train_lengths):.1f}, 90th percentile = {sorted(train_lengths)[int(0.9*len(train_lengths))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55da8de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train sample: {'text': \"<|im_start|>user\\n<time>2025-06-08T03:45:13</time>\\npls fast how do i convert like a 60 page word doc to latex latex i need latex i have to submit in the morning plz help<|im_end|>\\n<|im_start|>assistant\\nI can help you convert your Word document to LaTeX quickly! Don‚Äôt panic‚Äîyou can do this, even at 3:45 AM.\\n<think>\\nIt's very late at night (3:45 AM) and the user is clearly stressed and facing a tight deadline. Their grammar and typing suggest panic. This is likely a thesis or major assignment. My job is to calm them, give fast actionable steps, and avoid overwhelming details.\\n</think>\\nHere‚Äôs the fastest way to convert a large Word document (like 60 pages) to LaTeX:\\n\\n### 1. **Use an Online Converter (Fastest for Emergencies)**\\nTry [Pandoc](https://pandoc.org/)‚Äîit‚Äôs a free tool that can convert `.docx` to `.tex` files in one command.\\n\\n#### If you can use the command line:\\n```bash\\npandoc your_document.docx -o your_document.tex\\n```\\n- You‚Äôll need [Pandoc](https://pandoc.org/installing.html) installed. It works on Windows, Mac, or Linux.\\n- This converts most formatting‚Äîheadings, equations, lists, images, and references.\\n\\n#### If you need a web-based tool:\\n- Try [word2latex.com](https://word2latex.com/) or [docx2latex.com](https://docx2latex.com/).\\n- Upload your file and download the LaTeX version.\\n\\n### 2. **Check and Fix Formatting**\\n- Large docs may need some manual fixes (especially equations and tables).\\n- Open the `.tex` file in a LaTeX editor (like Overleaf, TeXstudio, or VS Code with a LaTeX extension).\\n- If you have special sections (bibliography, appendices), check them separately.\\n\\n### 3. **Don‚Äôt waste time retyping!**\\n- Always convert automatically first‚Äîthen do a quick sweep for any errors.\\n\\nStay calm. You still have time‚Äîfocus on getting a good conversion first, then tidy up the formatting.\\nIf you hit errors or have trouble compiling, just paste the error here and I‚Äôll help you fix it fast.<|im_end|>\\n<|im_start|>user\\n<time>2025-06-08T03:48:50</time>\\nhow to add the univ logo on title page like left top corner latex plzzz<|im_end|>\\n<|im_start|>assistant\\nHere‚Äôs how you can add your university or institute logo to the top-left of your title page in LaTeX:\\n<think>\\nThe user is still in a hurry and asking in a stressed, ungrammatical way. They need a copy-paste solution for inserting a logo, and it should work regardless of the LaTeX template. I‚Äôll use \\\\includegraphics from the graphicx package, and show where to place it.\\n</think>\\nMake sure you have your logo file (for example, `logo.png`) in the same directory as your `.tex` file.\\n\\nAdd this near the very start of your document (after `\\\\begin{document}` and before your title):\\n\\n```latex\\n\\\\usepackage{graphicx} % in preamble, if not already there\\n\\n% In the document body\\n\\\\begin{flushleft}\\n  \\\\includegraphics[width=3cm]{logo.png}\\n\\\\end{flushleft}\\n```\\n\\n- Replace `logo.png` with your logo file name.\\n- Adjust the `width` as needed.\\n- Place this **before** your title or main content so it appears at the top left.\\n\\nIf you need the logo exactly on the title page or want it to appear only there, put this code just before or after your `\\\\maketitle` command, or inside a custom title block.\\n\\nLet me know if your document uses a specific template or class and I can give you more exact placement instructions. Good luck‚Äîyou‚Äôre almost there!<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Format for TRL SFTTrainer: list of dicts with a 'text' field\n",
    "train_dataset = [{\"text\": x} for x in train_texts]\n",
    "\n",
    "print(f\"First train sample: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aae6f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 128\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Wrap the data in HuggingFace Datasets objects\n",
    "train_dataset = Dataset.from_list(train_dataset)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4251cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.12.8: Fast Qwen3 patching. Transformers: 4.57.3. vLLM: 0.13.0.\n",
      "   \\\\   /|    NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Num GPUs = 1. Max memory: 95.592 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15baebde8e4e4f92adf0e06290e4fbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name+\"phase3\",\n",
    "    max_seq_length = max_train_len,       # Highest sequence length in train dataset\n",
    "    load_in_4bit = True,                  # QLoRA\n",
    "    load_in_8bit = False,                 # We are using 4-bit\n",
    "    full_finetuning = False,              # QLoRA/PEFT not full FT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3eed904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.12.8 patched 40 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,              # Rank of LoRA\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=32,     # Standard\n",
    "    lora_dropout=0.05, # Standard\n",
    "    bias=\"none\",       # Standard\n",
    "    random_state = seed,\n",
    "    use_gradient_checkpointing=True  # Saves memory, a bit slower\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88dce9c",
   "metadata": {},
   "source": [
    "### Trainer Setup and Epoch-Level Checkpoint Strategy\n",
    "\n",
    "We configure the trainer to operate in a high-intensity alignment regime:\n",
    "\n",
    "- **Small batch (8) with large gradient accumulation (16)** yields an effective batch size of 128.\n",
    "- **Max steps** is empirically tuned per model size to ensure that one **epoch checkpoint** lands in the narrow **loss band of 1.045‚Äì1.05**, which we found indicative of optimal alignment without collapse.\n",
    "- **Learning rate (1.5e-4)** is increased to compensate for the short training horizon.\n",
    "- **Epoch-level saving** (`save_strategy=\"epoch\"`) enables **manual checkpoint selection**, as automated validation is disabled and the risk of overshooting this narrow zone is high.\n",
    "- **No evaluation set** is used in this phase, we will choose the first checkpoint in the **range of 1.045‚Äì1.05**.\n",
    "\n",
    "This configuration allows fine-grained control over the **transition to minimal reasoning**, permitting us to recover the one ‚Äújust right‚Äù checkpoint‚Äîneither underfit nor behaviorally degenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88dac55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2f62f3ef9b469e93076d44a897c2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,    # \n",
    "        gradient_accumulation_steps = 16,   # Effective batch size = 128\n",
    "        warmup_steps = 6,                   # \n",
    "        max_steps = 36,                     # \n",
    "        learning_rate = 1.5e-4,             # Higher LR as we have expotentially fewer steps\n",
    "        logging_steps = 1,                  # Log every step\n",
    "        optim = \"adamw_8bit\",               # Standard for QLoRA\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = seed,\n",
    "        report_to = \"none\",                 \n",
    "        max_grad_norm = 1.0,                # Standard\n",
    "        save_strategy = \"epoch\",            # Saving every epoch so we can test each checkpoint manually\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d4357e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition. Max memory = 95.592 GB.\n",
      "13.334 GB of memory reserved at start.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved at start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c08948c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 128 | Num Epochs = 36 | Total steps = 36\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 16 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 128,450,560 of 14,896,757,760 (0.86% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 33:50, Epoch 36/36]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.117800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.093700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.978300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.970900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.968700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory used during training: 26.354 GB.\n"
     ]
    }
   ],
   "source": [
    "# Begin training, collect stats\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# After training, report peak memory reserved\n",
    "peak_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "print(f\"Peak GPU memory used during training: {peak_gpu_memory} GB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79919639",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f801bf7-c038-40b7-b2a9-1dbdf001268d",
   "metadata": {},
   "source": [
    "### Rename Output Directory for Checkpoint Organization\n",
    "\n",
    "After training, we rename the default `trainer_output` directory to reflect the model size (e.g., `trainer_output14B`). This ensures that checkpoints from different model variants are isolated and clearly labeled for downstream merging and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e62574e-b745-438b-99cf-2f73ea2cf13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('trainer_output14B')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"trainer_output\").rename(\"trainer_output\"+model_name[11:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6f472-9f73-41db-9f10-d01d5c8918f8",
   "metadata": {},
   "source": [
    "### Checkpoint Selection Strategy and Model Merging\n",
    "\n",
    "The final checkpoint used for producing the **TIME** model is selected based on a *loss-based selection heuristic*, rather than arbitrary epoch limits or fixed training durations. Specifically, we monitor training loss and identify the **first epoch checkpoint** where the loss falls within a narrow empirically determined ‚Äúsweet spot‚Äù of **1.050 to 1.045**.\n",
    "\n",
    "This range has been established across multiple model sizes and curriculum phases as the inflection point that balances sufficient learning and structure acquisition without overfitting or collapse. Checkpoints before this range often show underdeveloped behavior (e.g., weaker temporal behavior and output formatting), while those after tend to fall into **degenerate  modes**‚Äîhallmarked by sharp increase in format bleed with markdown leaking into think blocks, think block content appearing outside think blocks, infinite repetitions and so on.\n",
    "\n",
    "This approach provides two key benefits:\n",
    "1. **Behavioral Stability**: The checkpoint selected within this band consistently shows high structural fidelity (clean reasoning boundaries, minimal format bleed), compact and economical reasoning, and robust temporal understanding across sizes.\n",
    "2. **Reproducibility**: Due to fixed seeds (`set_seed`, `random.seed`, `numpy.random.seed`) and deterministic inputs in the batch, **the exact same checkpoint is consistently produced** for a given model size and configuration, regardless of when or where it is trained (as long as training hyperparameters are unchanged). Despite underlying hardware non-determinism (e.g., CUDA kernels), the global training trajectory is sufficiently stable for the sweet spot checkpoint to remain invariant.\n",
    "\n",
    "Checkpoints are saved at the end of each epoch (`save_strategy = \"epoch\"`), and we can see from the training loss that the 24th checkpoint (i.e., epoch corresponding to training step 24) has training loss (**1.0496**) falling the sweet range. This might differ for other model sizes, but the procedure remains the same.\n",
    "\n",
    "After selection, we:\n",
    "- Reload the adapter and base model.\n",
    "- Merge LoRA adapters using `PeftModel.merge_and_unload()`.\n",
    "- Save the fully merged model under the `TIME` naming convention (e.g., `TIME-14B`).\n",
    "This model is then ready for downstream inference and benchmarking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c21b9605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdcf4e4bc7c4848ac2561d154f1af46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name+\"phase3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name+\"phase3\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d719bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TIME-14B/tokenizer_config.json',\n",
       " 'TIME-14B/special_tokens_map.json',\n",
       " 'TIME-14B/chat_template.jinja',\n",
       " 'TIME-14B/vocab.json',\n",
       " 'TIME-14B/merges.txt',\n",
       " 'TIME-14B/added_tokens.json',\n",
       " 'TIME-14B/tokenizer.json')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, \"trainer_output\"+model_name[11:]+\"/checkpoint-24\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"TIME\"+model_name[10:])\n",
    "tokenizer.save_pretrained(\"TIME\"+model_name[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c4e961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
